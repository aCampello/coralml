{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Todo\n",
    "\n",
    "- [x] Run the same colab as Pytorch\n",
    "- [x] Code for cutting the data (data prep)\n",
    "- [x] Code for creating masks\n",
    "- [x] Code for visualising masks\n",
    "- [ ] Build dataset class\n",
    "- [ ] Remove deeplab head\n",
    "- [ ] Trainer function\n",
    "- [ ] Run on some small data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download TorchVision repo to use some files from\n",
    "# references/detection\n",
    "!git clone https://github.com/pytorch/vision.git\n",
    "\n",
    "!cp vision/references/segmentation/utils.py .\n",
    "!cp vision/references/segmentation/transforms.py .\n",
    "!cp vision/references/segmentation/train.py .\n",
    "!cp vision/references/segmentation/coco_utils.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install cython\n",
    "# Install pycocotools\n",
    "\n",
    "!pip3 install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-08T23:53:30.111865Z",
     "iopub.status.busy": "2020-12-08T23:53:30.111627Z",
     "iopub.status.idle": "2020-12-08T23:53:30.121299Z",
     "shell.execute_reply": "2020-12-08T23:53:30.120022Z",
     "shell.execute_reply.started": "2020-12-08T23:53:30.111838Z"
    }
   },
   "source": [
    "## 1. Just using deeplab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'deeplabv3_resnet101', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = \"../data/images_val/02_2017_0803_132452_045.jpg\"\n",
    "print(\"Opening\")\n",
    "input_image = Image.open(filename)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "print(\"Pre-processing\")\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "print(\"Predicting\")\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)['out'][0]\n",
    "output_predictions = output.argmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "            'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "             'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a color pallette, selecting a color for each class\n",
    "palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n",
    "colors = torch.as_tensor([i for i in range(21)])[:, None] * palette\n",
    "colors = (colors % 255).numpy().astype(\"uint8\")\n",
    "\n",
    "# plot the semantic segmentation predictions of 21 classes in each color\n",
    "r = Image.fromarray(output_predictions.byte().cpu().numpy()).resize(input_image.size)\n",
    "r.putpalette(colors)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(r)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Code to crop images\n",
    "\n",
    "This is a code to crop images into specific rectangles (3,4) by default. Skip if you have done it already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Image.open(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '../data/images_val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob.glob(os.path.join(folder, '*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image.getdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def prepare_crops(folder, cropped_folder='data/cropped',\n",
    "                 crops=(4, 3)):\n",
    "    # Load\n",
    "    # Crop\n",
    "    # Calculate masks\n",
    "    images = glob.glob(os.path.join(folder, '*.jpg')) + glob.glob(os.path.join(folder, '*.JPG'))\n",
    "    images += glob.glob(os.path.join(folder, '*.png'))\n",
    "    images += glob.glob(os.path.join(folder, '*.PNG'))\n",
    "                 \n",
    "    os.makedirs(cropped_folder, exist_ok=True)\n",
    "    \n",
    "    for image_path in tqdm.tqdm(images):\n",
    "        filename = os.path.basename(image_path)\n",
    "\n",
    "        im = Image.open(image_path)\n",
    "        crop_x = im.size[0]//crops[0]\n",
    "        crop_y = im.size[1]//crops[1]\n",
    "\n",
    "        for i in range(crops[0]):\n",
    "            for j in range(crops[1]):\n",
    "                im.crop([crop_x*i, crop_y*j, crop_x*(i+1), crop_y*(j+1)]).save(\n",
    "                    os.path.join(cropped_folder, f'{filename[:-4]}_cropped_{i}{j}{filename[-4:]}')\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    'c_hard_coral_branching', \n",
    "    'c_hard_coral_submassive', \n",
    "    'c_hard_coral_boulder',\n",
    "    'c_hard_coral_encrusting', \n",
    "    'c_hard_coral_table', \n",
    "    'c_hard_coral_foliose',\n",
    "    'c_hard_coral_mushroom', \n",
    "    'c_soft_coral', \n",
    "    'c_soft_coral_gorgonian', \n",
    "    'c_sponge', \n",
    "    'c_sponge_barrel', \n",
    "    'c_fire_coral_millepora', \n",
    "    'c_algae_macro_or_leaves'\n",
    "]\n",
    "\n",
    "name_to_id = {y: x for x, y in enumerate(classes, start=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_masks(images_folder='../data/images_val/',\n",
    "                  masks_folder='data/masks',\n",
    "                  annotations_file='../data/imageCLEFcoral2020_GT.csv',\n",
    "                  cropped_folder='data/cropped'):\n",
    "    image_to_annotations = defaultdict(list)\n",
    "    \n",
    "    os.makedirs(masks_folder, exist_ok=True)\n",
    "    \n",
    "    with open(annotations_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line_split = line.split(' ')\n",
    "            image_path = os.path.join(images_folder, line_split[0] + '.JPG')\n",
    "        \n",
    "            if not os.path.exists(image_path):\n",
    "                image_path = os.path.join(images_folder, line_split[0] + '.jpg')\n",
    "            \n",
    "            substrate = line_split[2]\n",
    "            polygon = [int(x) for x in line_split[4:]]\n",
    "            # Polygons are pairs of points\n",
    "            polygon = [(x, y) for x, y in zip(polygon[::2], polygon[1::2])]\n",
    "\n",
    "            image_to_annotations[image_path] += [(substrate, polygon)]\n",
    "    \n",
    "    for image in tqdm.tqdm(image_to_annotations.keys(), total=len(image_to_annotations)):\n",
    "        filename = os.path.basename(image)\n",
    "        im_size = Image.open(image).size\n",
    "        # Creates a uint8 PNG\n",
    "        poly = Image.new('L', size=im_size)\n",
    "        pdraw = ImageDraw.Draw(poly)\n",
    "        for substrate, polygon in image_to_annotations[image]:\n",
    "            pdraw.polygon(polygon, fill=name_to_id[substrate])\n",
    "            \n",
    "        poly.save(os.path.join(masks_folder, filename[:-4] + '.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prepare_masks()\n",
    "prepare_crops('../data/images_val', cropped_folder='data/cropped/images')\n",
    "prepare_crops('data/masks', cropped_folder='data/cropped/masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_mask(image_key='02_2017_0803_132446_043', masks_folder='data/masks'):\n",
    "    colours = [(0, 0, 0), \n",
    "               (245, 185, 95), \n",
    "               (50, 50, 50),\n",
    "               (65, 50, 230),\n",
    "               (73, 74, 74),\n",
    "               (78, 252, 5), \n",
    "               (186, 153, 255), \n",
    "               (200, 103, 5), \n",
    "               (198, 5, 252),\n",
    "               (84, 194, 27), \n",
    "               (20, 145, 245),\n",
    "               (16, 133, 16), \n",
    "               (190, 234, 98),\n",
    "               (255, 233, 72)]\n",
    "    \n",
    "    #Â Linearises palette (because that's what PIL likes)\n",
    "    colours_int = [x for y in colours for x in y]\n",
    "\n",
    "    mask = Image.open(os.path.join(masks_folder, image_key + '.png'))\n",
    "    mask.putpalette(colours_int)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_mask('02_2017_0803_132446_043')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_key='02_2017_0803_132446_043'\n",
    "masks_folder='data/'\n",
    "mask = Image.open(os.path.join(masks_folder, image_key + '.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class CoralDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_folder='data/cropped/images',\n",
    "                 masks_folder='data/cropped/masks',\n",
    "                 transforms=None,\n",
    "                 n_images=None):\n",
    "        self.images_folder = images_folder\n",
    "        self.masks_folder = masks_folder\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(sorted(os.listdir(images_folder)))[:n_images]\n",
    "        self.masks = list(sorted(os.listdir(masks_folder)))[:n_images]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        img_path = os.path.join(self.images_folder, self.imgs[idx])\n",
    "        mask_path = os.path.join(self.masks_folder, self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        target = Image.open(mask_path)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define model\n",
    "\n",
    "Remove HEAD from last layers and add fresh ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'deeplabv3_resnet101', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torchvision.models.segmentation.fcn import FCNHead\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get in channels form the convolutional layer\n",
    "\n",
    "in_channels_head = model.classifier[0].convs[0][0].in_channels\n",
    "in_channels_aux = model.aux_classifier[0].in_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = DeepLabHead(in_channels=in_channels_head, num_classes=14)\n",
    "model.aux_classifier = FCNHead(in_channels=in_channels_aux, channels=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from train import train_one_epoch, get_transform, criterion, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CoralDataset(transforms=get_transform(train=True), n_images=5)\n",
    "dataset_test = CoralDataset(transforms=get_transform(train=False), n_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "test_sampler = torch.utils.data.SequentialSampler(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2,\n",
    "    sampler=train_sampler, \n",
    "    collate_fn=utils.collate_fn, drop_last=True\n",
    ")\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1,\n",
    "    sampler=test_sampler,\n",
    "    collate_fn=utils.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 14\n",
    "epochs = 10\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params_to_optimize = [\n",
    "    {\"params\": [p for p in model.backbone.parameters() if p.requires_grad]},\n",
    "    {\"params\": [p for p in model.classifier.parameters() if p.requires_grad]},\n",
    "    {\"params\": [p for p in model.aux_classifier.parameters() if p.requires_grad], \"lr\": 0.005*10}\n",
    "]\n",
    "optimizer = torch.optim.SGD(params_to_optimize, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# and a learning rate scheduler which decreases the learning rate by\n",
    "# 10x every 3 epochs\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lambda x: (1 - x / (len(data_loader) * epochs)) ** 0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, 3):\n",
    "    train_one_epoch(model, criterion, optimizer, data_loader, lr_scheduler, device, epoch=epoch, print_freq=2)\n",
    "    confmat = evaluate(model, data_loader_test, device=device, num_classes=num_classes)\n",
    "    \n",
    "    print(confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualise model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms \n",
    "\n",
    "filename = \"data/cropped/images/02_2017_0803_132446_043_cropped_00.jpg\"\n",
    "print(\"Opening\")\n",
    "input_image = Image.open(filename)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "print(\"Pre-processing\")\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "# move the input and model to GPU for speed if available\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    model.to('cuda')\n",
    "\n",
    "print(\"Predicting\")\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)['out'][0]\n",
    "output_predictions = output.argmax(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
